# Guide d'ExÃ©cution : Job Spark sur le Cluster

## ğŸ¯ Objectif

Soumettre le job PySpark `spark_traffic_processing.py` au cluster Spark Master via `spark-submit`.

---

## ğŸ“‹ PrÃ©requis

1. **Stack Docker en cours** :

   ```powershell
   docker compose ps
   ```

   VÃ©rifiez que `spark-master` et `spark-worker` sont **Up**.
2. **DonnÃ©es HDFS prÃ©sentes** :

   ```powershell
   docker exec -it namenode hdfs dfs -ls /user/hdfs/traffic/year=2026/month=01/day=11
   ```

   Vous devez voir des fichiers `.jsonl`.

---

## ğŸš€ MÃ©thode 1 : Soumission depuis l'hÃ´te (Windows)

### 1ï¸âƒ£ Copier le script dans le conteneur Spark Master

```powershell
docker cp scripts/spark_traffic_processing.py spark-master:/tmp/
```

### 2ï¸âƒ£ Soumettre le job avec `spark-submit`

```powershell
docker exec -it spark-master /opt/spark/bin/spark-submit `
    --master spark://spark-master:7077 `
    --deploy-mode client `
    --executor-memory 2g `
    --total-executor-cores 2 `
    /tmp/spark_traffic_processing.py
```

**Explications des options** :

- `--master` : URL du cluster Spark Master
- `--deploy-mode client` : le driver tourne dans le conteneur master (pas sur un worker)
- `--executor-memory` : mÃ©moire allouÃ©e par executor
- `--total-executor-cores` : nombre total de cÅ“urs utilisÃ©s

---

## ğŸš€ MÃ©thode 2 : Soumission depuis le conteneur Spark Master

### 1ï¸âƒ£ Entrer dans le conteneur

```powershell
docker exec -it spark-master bash
```

### 2ï¸âƒ£ Copier le script (si pas dÃ©jÃ  fait)

```bash
# Depuis l'hÃ´te Windows (PowerShell)
docker cp scripts/spark_traffic_processing.py spark-master:/opt/spark/work-dir/
```

### 3ï¸âƒ£ Soumettre le job

```bash
cd /opt/spark/work-dir
/opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --executor-memory 2g \
    --total-executor-cores 2 \
    spark_traffic_processing.py
```

---

## ğŸ“Š VÃ©rification des RÃ©sultats

###### 1ï¸âƒ£ VÃ©rifier les fichiers Parquet (analytics)

```powershell
# KPI par road_type
docker exec -it namenode hdfs dfs -ls /data/analytics/traffic/kpi_road_type

# KPI par zone (partitionnÃ©)
docker exec -it namenode hdfs dfs -ls /data/analytics/traffic/kpi_zone

# KPI par heure
docker exec -it namenode hdfs dfs -ls /data/analytics/traffic/kpi_hourly

# RÃ©partition congestion
docker exec -it namenode hdfs dfs -ls /data/analytics/traffic/kpi_congestion
```

### 2ï¸âƒ£ Lire un fichier Parquet (exemple)

```powershell
docker exec -it spark-master /opt/spark/bin/pyspark --master local[*]
```

Puis dans le shell PySpark :

```python
df = spark.read.parquet("hdfs://namenode:9000/data/analytics/traffic/kpi_road_type")
df.show()
```

### 3ï¸âƒ£ VÃ©rifier l'Ã©chantillon CSV

```powershell
docker exec -it namenode hdfs dfs -ls /data/processed/traffic

# PowerShell (lire les 20 premiÃ¨res lignes)
docker exec namenode hdfs dfs -cat /data/processed/traffic/part-00000-*.csv > results.csv
Get-Content results.csv -Head 20
```

---

## ğŸ› ï¸ DÃ©pannage

| ProblÃ¨me                             | Solution                                                                                |
| ------------------------------------ | --------------------------------------------------------------------------------------- |
| **`Connection refused` au namenode** | VÃ©rifiez que le namenode est**healthy** : `docker compose ps`                           |
| **`No such file or directory` HDFS** | VÃ©rifiez le chemin d'entrÃ©e :`docker exec -it namenode hdfs dfs -ls /user/hdfs/traffic` |
| **Job Spark bloquÃ©**                 | VÃ©rifiez les logs du worker :`docker logs spark-worker`                                 |
| **MÃ©moire insuffisante**             | RÃ©duire `--executor-memory` Ã  `1g` ou augmenter la RAM Docker                           |

---

## ğŸ“¦ Variables d'Environnement (optionnelles)

Si vous voulez personnaliser les chemins sans modifier le script :

```powershell
docker exec -it spark-master /opt/spark/bin/spark-submit `
    --master spark://spark-master:7077 `
    --deploy-mode client `
    --conf "spark.executorEnv.INPUT_PATH=hdfs://namenode:9000/user/hdfs/traffic/*/*/*/*/*.jsonl" `
    --conf "spark.executorEnv.OUTPUT_ANALYTICS_PATH=hdfs://namenode:9000/data/analytics/traffic" `
    /tmp/spark_traffic_processing.py
```

---

## ğŸ¯ RÃ©sultat Attendu

Ã€ la fin du job :

```
âœ… Spark Session crÃ©Ã©e : spark://spark-master:7077
ğŸ“‚ Lecture depuis : hdfs://namenode:9000/user/hdfs/traffic/*/*/*/*/*.jsonl
ğŸ“Š Lignes brutes lues : 1234
ğŸ§¹ AprÃ¨s nettoyage : 1234 lignes
ğŸ”„ AprÃ¨s dÃ©duplication : 1230 lignes
ğŸ’¾ DataFrame mis en cache
ğŸ“ˆ Calcul des KPIs...
âœ… KPI 1 : Vitesse moyenne par road_type
+-------------+------------------+------------+
|road_type    |avg_speed         |total_events|
+-------------+------------------+------------+
|autoroute    |85.3              |120         |
|avenue       |42.1              |450         |
|rue          |28.7              |660         |
+-------------+------------------+------------+
...
ğŸ’¾ Sauvegarde des rÃ©sultats...
âœ… Analytics sauvegardÃ©es en Parquet
âœ… Ã‰chantillon CSV sauvegardÃ©
âœ… Job Spark terminÃ© avec succÃ¨s !
```

---

## ğŸ”— Liens Utiles

- **Spark Master UI** : http://localhost:8080
- **Spark Worker UI** : http://localhost:8081
- **HDFS NameNode UI** : http://localhost:9870

**PrÃªt Ã  lancer le job ?** ğŸš€
